[
  {
    "objectID": "Resume.html",
    "href": "Resume.html",
    "title": "Resume",
    "section": "",
    "text": "You can view and download my resume."
  },
  {
    "objectID": "miniproject4.html",
    "href": "miniproject4.html",
    "title": "SQL Plotting",
    "section": "",
    "text": "In this project, I will reproduce the graph created in Figure 1 of the Voss (2020) study using data from the Wideband Acoustic Immittance (WAI) Database. The graph will plot frequency against mean absorption for the 12 studies referenced in Figure 1, showing the differences across studies. I will also explore a second study, Aithal (2013), focusing on differences in mean absorption by sex. Both analyses will involve querying the WAI Database with SQL to extract the necessary data and using R to create plots.\n\nlibrary(RMariaDB)\nlibrary(tidyverse)\nlibrary(ggplot2)\n\n\n\n# A tibble: 4,981,558 × 16\n   Identifier SubjectNumber Session Ear   Instrument   Age AgeCategory EarStatus\n   &lt;chr&gt;              &lt;int&gt;   &lt;int&gt; &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;    \n 1 Abur_2014              1       1 Left  HearID        20 Adult       Normal   \n 2 Abur_2014              1       1 Left  HearID        20 Adult       Normal   \n 3 Abur_2014              1       1 Left  HearID        20 Adult       Normal   \n 4 Abur_2014              1       1 Left  HearID        20 Adult       Normal   \n 5 Abur_2014              1       1 Left  HearID        20 Adult       Normal   \n 6 Abur_2014              1       1 Left  HearID        20 Adult       Normal   \n 7 Abur_2014              1       1 Left  HearID        20 Adult       Normal   \n 8 Abur_2014              1       1 Left  HearID        20 Adult       Normal   \n 9 Abur_2014              1       1 Left  HearID        20 Adult       Normal   \n10 Abur_2014              1       1 Left  HearID        20 Adult       Normal   \n# ℹ 4,981,548 more rows\n# ℹ 8 more variables: TPP &lt;dbl&gt;, AreaCanal &lt;dbl&gt;, PressureCanal &lt;dbl&gt;,\n#   SweepDirection &lt;chr&gt;, Frequency &lt;dbl&gt;, Absorbance &lt;dbl&gt;, Zmag &lt;dbl&gt;,\n#   Zang &lt;dbl&gt;\n\n\n\nSHOW TABLES;\n\n\n7 records\n\n\nTables_in_wai\n\n\n\n\nCodebook\n\n\nMeasurements\n\n\nMeasurements_pre2020\n\n\nPI_Info\n\n\nPI_Info_OLD\n\n\nSubjects\n\n\nSubjects_pre2020\n\n\n\n\n\n\nDESCRIBE Measurements;\n\n\nDisplaying records 1 - 10\n\n\nField\nType\nNull\nKey\nDefault\nExtra\n\n\n\n\nIdentifier\nvarchar(50)\nNO\nPRI\nNA\n\n\n\nSubjectNumber\nint\nNO\nPRI\nNA\n\n\n\nSession\nint\nNO\nPRI\nNA\n\n\n\nEar\nvarchar(50)\nNO\nPRI\n\n\n\n\nInstrument\nvarchar(50)\nNO\nPRI\n\n\n\n\nAge\nfloat\nYES\n\nNA\n\n\n\nAgeCategory\nvarchar(50)\nYES\n\nNA\n\n\n\nEarStatus\nvarchar(50)\nYES\n\nNA\n\n\n\nTPP\nfloat\nYES\n\nNA\n\n\n\nAreaCanal\nfloat\nYES\n\nNA\n\n\n\n\n\n\n\nSELECT *\nFROM Measurements\nLIMIT 0, 5;\n\n\n5 records\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentifier\nSubjectNumber\nSession\nEar\nInstrument\nAge\nAgeCategory\nEarStatus\nTPP\nAreaCanal\nPressureCanal\nSweepDirection\nFrequency\nAbsorbance\nZmag\nZang\n\n\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n210.938\n0.0333379\n113780000\n-0.233504\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n234.375\n0.0315705\n103585000\n-0.235778\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n257.812\n0.0405751\n92951696\n-0.233482\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n281.250\n0.0438399\n86058000\n-0.233421\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n304.688\n0.0486400\n79492800\n-0.232931\n\n\n\n\n\n\nSELECT DISTINCT Identifier\nFROM Measurements;\n\n\nDisplaying records 1 - 10\n\n\nIdentifier\n\n\n\n\nAbur_2014\n\n\nAithal_2013\n\n\nAithal_2014\n\n\nAithal_2014b\n\n\nAithal_2015\n\n\nAithal_2017a\n\n\nAithal_2017b\n\n\nAithal_2019a\n\n\nAithal_2019b\n\n\nAithal_2020a\n\n\n\n\n\n\n-- Select relevant data from the Measurements and PI_Info tables\nSELECT \n  m.Identifier,                                      -- Unique study identifier\n  CONCAT(AuthorsShortList, \" (\", YEAR, \") \", \n         \"N= \", COUNT(DISTINCT SubjectNumber, Ear), \n         \";\", Instrument) AS Label,                 -- Concatenate authors, year, sample size, and instrument details into a descriptive label\n  m.Frequency,                                      -- Frequency of measurement in Hz\n  AVG(m.Absorbance) AS Mean_Absorbance,             -- Calculate the mean absorbance for each study and frequency\n  COUNT(DISTINCT m.SubjectNumber, m.Ear) AS Unique_Ears -- Count the number of unique ears measured in the study\nFROM Measurements AS m\nJOIN PI_Info AS pi                                  -- Join Measurements table with PI_Info table\n  ON m.Identifier = pi.Identifier                   -- Match records by the study identifier\nWHERE m.Identifier IN (\n  'Abur_2014', 'Werner_2010', 'Voss_2020', \n  'Feeny_2017', 'Groon_2015', 'Lewis_2015', \n  'Liu_2008', 'Rosowski_2012', 'Shahnaz_2006', \n  'Shaver_2013', 'Sun_2016', 'Voss_1994', \n  'Voss_2010')                                      -- Filter for the specified 13 studies\n  AND m.Frequency &gt; 200                             -- Include only frequencies above 200 Hz\n  AND m.Frequency &lt; 8000                            -- Include only frequencies below 8000 Hz\nGROUP BY m.Identifier, Frequency, Instrument;       \n\n\nlibrary(scales)\n\nears |&gt;\n  ggplot(aes(x = Frequency, y = Mean_Absorbance, color = Label)) +\n  geom_line() +\n  xlim(200, 8000) +\n  ylim(0, 1) +\n  labs(\n    title = \"Mean absorbance from each publication in WAI database\",\n    x = \"Frequency(Hz)\",\n    y = \"Mean Absorbance\",\n    color = NULL\n  ) +\n  # proper x and y scales\n  scale_x_log10(\n    limits = c(200, 8000),\n    breaks = c(200, 400, 600, 800, 1000, 2000, 4000, 6000, 8000)\n  ) +\n  scale_y_continuous(\n    limits = c(0, 1),\n    breaks = seq(0, 1, 0.2)\n  ) +\n  theme_bw() +\n  \n  theme(\n    plot.background = element_rect(\n      fill = \"grey90\", \n      color = \"grey90\", \n      size = 30),\n    legend.position = c(0.01, 0.99),\n    legend.justification = c(0, 1),\n    legend.background = element_rect(\n      color = \"black\",\n      size = 0.3,\n      fill = \"white\"\n    ),\n    legend.text = element_text(size = 7),\n    legend.key.height = unit(0.15, \"mm\"),\n    legend.key = element_rect(size = 0.1),\n    legend.key.size = unit(1, \"cm\"),\n    aspect.ratio = 0.8,\n    text = element_text(face = \"bold\"),\n    plot.title = element_text(hjust = 0.5),\n    plot.margin = margin(12, 12, 12, 12)\n  )\n\n\n\n\n\n\n\n\nThis graphs analyzes 12 different studies from the WAI database. Each line represents data from a specific study, identified by the author, year, instrument, and the number of unique ears measured. The x-axis shows frequency in Hertz (Hz), ranging from 200 Hz to 8000 Hz, while the y-axis displays mean absorption values ranging from 0 to 1. This plot highlights variations in mean absorption across different studies and instruments, demonstrating how measurement techniques and populations may influence auditory outcomes. Each study has differing ranges in mean absorbency and frequency.\nNow we will reproduce a similar graph, finding a study in the same database where subjects of different sex. In doing this I will be using the Aithal (2013) study.\n\n-- Select relevant data from the Measurements and Subjects tables\nSELECT \n  m.Identifier,                                   -- Unique study identifier\n  m.Frequency,                                   -- Frequency of measurement in Hz\n  s.Sex,                                         -- Subject's sex (e.g., Male or Female)\n  AVG(m.Absorbance) AS Mean_Absorbance,          -- Calculate the mean absorbance for each frequency and sex group\n  COUNT(DISTINCT m.SubjectNumber, m.Ear) AS Unique_Ears -- Count the unique ears measured per frequency and sex\nFROM Measurements AS m\nJOIN Subjects AS s                               -- Join Measurements table with Subjects table\n  ON m.SubjectNumber = s.SubjectNumber           -- Match records by subject number\nWHERE m.Identifier = 'Aithal_2013'               -- Filter for the study \"Aithal_2013\" only\nGROUP BY m.Frequency, s.Sex                      -- Group results by frequency and subject sex for aggregation\nORDER BY m.Frequency, s.Sex;                     -- Sort results by frequency and then by sex\n\n\nSELECT Sex, Identifier\nFROM Subjects\n\n\n# Correcting the typo in the 'Sex' column\nsex_analysis &lt;- sex_analysis |&gt; \n  mutate(Sex = str_replace(Sex, \"\\\\bFemake\\\\b\", \"Female\"))\n\n\nsex_analysis |&gt;\n  ggplot(aes(x = Frequency, y = Mean_Absorbance, color = Sex)) +\n  geom_line() +\n  labs(\n    title = \"Frequency vs. Mean Absorbance by Sex for Aithal_2013\",\n    x = \"Frequency (Hz)\",\n    y = \"Mean Absorbance\",\n    color = \"Sex\"\n  ) +\n  scale_x_log10(\n    limits = c(200, 8000),\n    breaks = c(200, 400, 600, 800, 1000, 2000, 4000, 6000, 8000)\n  ) +\n  scale_y_continuous(\n    limits = c(0, 1),\n    breaks = seq(0, 1, 0.2)\n  ) +\n  theme_bw() +\n  \n  theme(\n    plot.background = element_rect(\n      fill = \"grey90\", \n      color = \"grey90\", \n      size = 30),\n    legend.position = c(0.01, 0.99),\n    legend.justification = c(0, 1),\n    legend.background = element_rect(\n      color = \"black\",\n      size = 0.3,\n      fill = \"white\"\n    ),\n    legend.text = element_text(size = 7),\n    legend.key.height = unit(0.15, \"mm\"),\n    legend.key = element_rect(size = 0.1),\n    legend.key.size = unit(1, \"cm\"),\n    aspect.ratio = 0.8,\n    text = element_text(face = \"bold\"),\n    plot.title = element_text(hjust = 0.5),\n    plot.margin = margin(12, 12, 12, 12)\n  )\n\n\n\n\n\n\n\n\nThis graphs analyzes just the Aithal (2013) study from the WAI database. Each line represents data from a specific sex group. There seems to be some mistake in the data where “Female” was spelled “Femake”. The x-axis shows frequency in Hertz (Hz), ranging from 200 Hz to 8000 Hz, while the y-axis displays mean absorption values ranging from 0 to 1. This plot highlights that there are not distinct variations in mean absorption and frequency across different sex’s, demonstrating that sex does not influence auditory outcomes.\nData Source:\nThe data used in this analysis comes from the Wideband Acoustic Immittance (WAI) Database, hosted by Smith College. The WAI Database contains a collection of auditory measurements provide measurements across multiple studies and demographic groups. The database includes data from various peer-reviewed studies, including those focusing on different age, sex, and ethnicity groups.\nCitation:\nVoss:\nVoss, Susan E. Ph.D. Resource Review. Ear and Hearing 40(6):p 1481, November/December 2019. | DOI: 10.1097/AUD.0000000000000790\nAithal:\nAithal, V., Kei, J., Driscoll, C., & Khan, A. (2013). Normative wideband reflectance in healthy neonates: A cross-sectional study. Journal of the American Academy of Audiology, 24(9), 832-842.\n\n# Disconnect from the database\ndbDisconnect(con_wai)"
  },
  {
    "objectID": "miniproject2.html",
    "href": "miniproject2.html",
    "title": "Text Analysis",
    "section": "",
    "text": "In this project, I analyzed the Netflix Movies and TV Shows dataset to explore trends and insights into the platform’s content. This dataset, sourced from Kaggle and made available through TidyTuesday, provides information on titles, genres, release years, ratings, and descriptions.\n\nnetflix_titles &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-04-20/netflix_titles.csv')\n\n\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(tidytuesdayR)\nlibrary(stringr)\n\n\n# changed all captitalized letters to lower in order to analyze\nnetflix_titles2 &lt;- netflix_titles |&gt;\n  mutate(listed_in = str_replace_all(listed_in, \"[A-Z]\", tolower))\n\n\n# Chose 6 main genres to examine\ngenre_titles &lt;- netflix_titles2 |&gt;\n  filter(str_detect(listed_in, \"comedy|comedies|drama|horror|documentary|action|crime|reality\")) |&gt;\n  mutate(genre = case_when(\n    str_detect(listed_in, \"comedy|comedies\") ~ \"Comedy\",\n    str_detect(listed_in, \"drama\") ~ \"Drama\",\n    str_detect(listed_in, \"horror\") ~ \"Horror\",\n    str_detect(listed_in, \"documentary\") ~ \"Documentary\",\n    str_detect(listed_in, \"action\") ~ \"Action\",\n    str_detect(listed_in, \"crime\") ~ \"Crime\",\n    str_detect(listed_in, \"reality\") ~ \"Reality\"\n  )) |&gt;\n  select(genre, type)\n\n\ngenre_plot &lt;-ggplot(genre_titles, aes(x = genre)) +\n    geom_bar() +\n    labs(\n      x = \"Genre\",\n      y = \"Count\",\n      title = \"# of Movies per Genre\"\n    )\nprint(genre_plot)\n\n\n\n\n\n\n\n\n\nlibrary(plotly)\n\n# Creating the Plotly interactive plot for genre distribution\ngenre_plotly &lt;- genre_titles |&gt;\n  count(genre) |&gt;\n  plot_ly(x = ~genre, y = ~n, type = 'bar', \n          text = ~paste('Count: ', n), \n          hoverinfo = 'text') |&gt;\n  layout(title = \"Number of Movies per Genre\",\n         xaxis = list(title = \"Genre\"),\n         yaxis = list(title = \"Count\"))\n\n# Display the interactive plot\ngenre_plotly\n\n\n\n\n\n\n# create data frame that counts amount of titles in within each year\nyear_counts &lt;- netflix_titles |&gt;\n  group_by(release_year, type) |&gt;\n  summarise(count = n()) |&gt;\n  arrange(desc(count))\n\n\nyear_plot &lt;- \n  ggplot(year_counts, aes(x = release_year , y = count, color = type)) +\n  geom_point() +\n  labs(title = \"Number of Titles by Genre\", \n       x = \"Genre\", \n       y = \"Count\"\n       )\nprint(year_plot)\n\n\n\n\n\n\n\n\n\n# calculates length of descriptions\ndescription_length &lt;- netflix_titles |&gt;\n  mutate(description_length = str_length(description)) |&gt;\n  select(title, description, description_length) |&gt;\n  arrange(desc(description_length))\n\n\n# calculates average length of descriptions\nmean_description_length &lt;- description_length |&gt;\n  summarize(mean_length = mean(description_length, na.rm = TRUE))\n\nmean_description_length\n\n# A tibble: 1 × 1\n  mean_length\n        &lt;dbl&gt;\n1        143.\n\n\n\n# tibble with just rating and movie title\nrating &lt;- netflix_titles |&gt;\n  filter(str_detect(rating, \"^(R|PG-13|PG|G)$\")) |&gt;\n  select(title, rating)\nrating\n\n# A tibble: 1,337 × 2\n   title       rating\n   &lt;chr&gt;       &lt;chr&gt; \n 1 23:59       R     \n 2 9           PG-13 \n 3 21          PG-13 \n 4 187         R     \n 5 3022        R     \n 6 22-Jul      R     \n 7 Æon Flux    PG-13 \n 8 10,000 B.C. PG-13 \n 9 13 Sins     R     \n10 14 Blades   R     \n# ℹ 1,327 more rows\n\n\nAnalysis on Tables/Plots:\nMy first table, I wanted to change the genre descriptions to all lowercase. I did this so that for my next table I could condense all of the genres into six different categories, allowing me to plot a graph showing the amount of movies/television shows in each genre.\nMy first graph represents that amount of movies/television shows in each genre. From this graph, we can see the most common genres are comedy and drama. The least common genres are crime and reality.\nI upgraded my first graph by producing a interactive plot with plotly.\nFrom my third graph we can see the relationship between year, and movie/television show release date. We can infer from this graph that that a lot more movies and television shows were put on Netflix after the mid 2000s.\nI wanted to play around with the str_length function. So I took the length of all of the descriptions of each title. The mean of the description lengths was 143.1004 string characters.\nLastly, I created a new data frame that takes the title of the show and detects the R, PG-13, PG and G ratings\nData Source:\nNetflix Movies and TV Shows Dataset\n\nAuthor: Shivam Bansal\nThis data was sourced from Kaggle and contains information about various movies and TV shows available on Netflix. Specifically, it includes:\n\nAttributes such as title, genre, language, release year, and rating for each movie or TV show.\nThe dataset offers insights into Netflix’s catalog, making it useful for analyses such as trends in content release, genre popularity, and the evolution of Netflix’s original content over time.\nThe data was last updated on Kaggle by Shivam Bansal."
  },
  {
    "objectID": "food.html",
    "href": "food.html",
    "title": "Food Consumption",
    "section": "",
    "text": "In this project, I analyze data from the TidyTuesday initiative, focusing on food consumption and its associated carbon dioxide (CO2) emissions. The dataset provides insights into the relationship between different types of food consumption and their environmental impact. By visualizing this relationship, I aim to identify patterns and highlight how dietary choices contribute to CO2 emissions, a critical factor in discussions on climate change and sustainability.\n\nfood &lt;- read.csv(\"food_consumption.csv\")\n\nlibrary(tidyverse)\nlibrary(ggplot2)\n\n# generating plot, co2 emmisions on x-axis, consumption on y-axis\nggplot(food, aes(x = co2_emmission, y = consumption)) +\n  geom_point() +\n  labs(\n    x = \"CO2 Emissions\",\n    y = \"Consumption\",\n    title = \"Relationship food consumption and CO2 emissions\",\n  )\n\n\n\n\n\n\n\n\nUsing the dataset food_consumption.csv, I created a scatter plot that depicts the relationship between food consumption levels (measured in kilograms per capita) and CO2 emissions (measured in kilograms of CO2 equivalent). This plot serves to illustrate whether foods with higher consumption rates also tend to have higher associated CO2 emissions.\nData Source:\nhttps://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-05-26/all_drinks.csv\n\nAuthor: TidyTuesday Project\nThis data was sourced from the TidyTuesday project, which focuses on weekly data challenges for the R community. Specifically, the data pertains to food consumption by different drinks:\n\nThe dataset was updated on May 26, 2020.\nIt includes data related to the consumption of various drinks, with a focus on their distribution and popularity across different regions.\nThe data spans various food and drink categories and provides detailed consumption statistics."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\nChloe Denhart\n",
    "section": "",
    "text": "Chloe Denhart\n\n\nAbout this site:\nWelcome to my website! I am a student at Pomona College majoring in Economics and minoring in Data Science and Math. At Pomona I am an Analyst on the technology team for Saghen Capital Management, and an Associate Consultant for Pomona Consulting Group. I also play on the Pomona Woman’s Varsity Lacrosse team."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Chloe Denhart",
    "section": "",
    "text": "About this site:\nWelcome to my website! I am a student at Pomona College majoring in Economics and minoring in Data Science. I am a member of the Pomona Woman’s Lacrosse team. Outside of Pomona, I am an avid camper and hiker!"
  },
  {
    "objectID": "drinks.html",
    "href": "drinks.html",
    "title": "All Drinks",
    "section": "",
    "text": "In this project, I analyze the distribution of drink categories using the TidyTuesday All Drinks dataset. The dataset provides detailed information about various drink types and their associated categories, which makes it ideal for exploring trends and understanding the popularity of different beverages.\n\ndrinks &lt;- read.csv(\"all_drinks.csv\")\n\n\nlibrary(tidyverse)\nlibrary(ggplot2)\n\nggplot(drinks, aes(x = strCategory)) +\n  geom_bar() +\n  labs(\n    x = \"Drink Category\",\n    y = \"Count\",\n    title = \"Analysis of amount of drinks in each drink category\",\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nHere I created a bar plot that visualizes the number of drinks in each category. The x-axis represents the drink categories, while the y-axis displays their respective counts. To enhance readability, I rotated the category labels on the x-axis.\nThis visualization helps identify which drink categories are most common in the dataset and highlights trends that could inform further research or practical applications in food and beverage studies.\nData Source:\nhttps://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-02-18/food_consumption.csv\n\nAuthor: TidyTuesday Project\nThis data was sourced from the TidyTuesday project, which provides weekly data challenges for the R community. Specifically, the dataset focuses on food consumption:\n\nThe dataset was updated on February 18, 2020.\nIt includes information on food consumption across different categories and regions, focusing on various food and drink items.\nThe first column of the dataset typically provides the year for each data point."
  },
  {
    "objectID": "miniproject3.html",
    "href": "miniproject3.html",
    "title": "Permutation Tests",
    "section": "",
    "text": "The code here will provide an analysis on March Madness data. March madness is one of the most unpredictable tournaments in sports, as lower seeded teams are constantly “upsetting” higher seeded teams. The analysis done here will tackle the actual probability of lower seeded teams beating higher seeded teams. Our research question is: What is the probability of a lower-seeded team winning against a higher-seeded team in the first round of the 2023 NCAA March Madness tournament, and does this probability exceed 90%? To determine this I will conduct a permutation test with the hypothesis:\n\nNull hypothesis (H0) : the probability that a lower seeded team wins being less than or equal to 90% (Upset win rate ≤ 90%).\nAlternative hypothesis (Ha): the probability that a lower seeded team wins being greater than 90% (Upset win rate &gt; 90%)\n\nIn this case, we will be using a right tailed permutation test. If the p-value is below the significance level (0.05), we reject the null hypothesis, if it is not, we will fail to reject the null hypothesis.\n\nmarch_madness &lt;- read.csv(\"538 Ratings.csv\")\n\nwalk through these lines of code\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(purrr)\n\n\n# Filter for games from the 2023 tournament, specifically the first round (Round of 64)\n# Count the total number of games played in this round\ntotal_games &lt;- march_madness |&gt;\n  filter(YEAR == 2023, ROUND == 64) |&gt;\n  nrow()\ntotal_games # Output the total number of games\n\n[1] 32\n\n\n\n# Filter for lower-seeded teams (seed &gt; 8) winning in earlier rounds\nupsets &lt;- march_madness |&gt;\n  filter(YEAR == 2023, ROUND &lt; 64, SEED &gt; 8) |&gt;\n  nrow()\nupsets # Output the number of upsets\n\n[1] 7\n\n\n\n# Create a summary table to display results by seed group and outcome\nmarch_madness |&gt;\n  filter(YEAR == 2023) |&gt;\n  filter(ROUND &lt;= 64) |&gt;\n  mutate(RESULT = ifelse(ROUND &lt; 64, \"win\", \"loss\")) |&gt;\n  mutate(seed_group = ifelse(SEED &lt;= 8, \"top\", \"bottom\")) |&gt;\n  group_by(seed_group, RESULT) |&gt;\n  summarize(n())\n\n# A tibble: 4 × 3\n# Groups:   seed_group [2]\n  seed_group RESULT `n()`\n  &lt;chr&gt;      &lt;chr&gt;  &lt;int&gt;\n1 bottom     loss      25\n2 bottom     win        7\n3 top        loss       7\n4 top        win       25\n\n# The upset rate is 0.21875 (7/32)\n\n\n# upset rate drawn from the total number of games in round of 64, and the amount of games won by a team when their seed &gt;= 8. See table above.\nupset_rate &lt;- upsets/total_games\nupset_rate\n\n[1] 0.21875\n\n\n\n# Define a function to simulate the upset ratio under the null hypothesis\nrandom_choice &lt;- function(num_teams) {\n  # Simulate outcomes based on probabilities (90% lower, 10% higher)\n  shuffled_seed &lt;- sample(c(\"lower\", \"higher\"), num_teams, replace = TRUE, prob = c(0.9, 0.1))\n  # Calculate the proportion of higher seeds winning\n  upset_ratio &lt;- mean(shuffled_seed == \"higher\")\n  return(upset_ratio)\n}\n\n\n# Set seed for reproducibility\nset.seed(47)\n\n# Number of simulations\nnum_exper &lt;- 5000\n\n# Generate null distribution using the simulation function\nnull_distribution &lt;- map_dbl(1:num_exper, ~ random_choice(total_games))\n\n\n# Calculate the p-value\n# Proportion of simulated upset rates greater than or equal to the observed upset rate\np_value &lt;- sum(null_distribution &gt;= upset_rate) / num_exper\np_value # Output the p-value\n\n[1] 0.035\n\n\n\n# Plot the null distribution with the observed upset rate\nggplot(data.frame(null_distribution), aes(x = null_distribution)) + \n  geom_histogram(bins = 30, color = \"black\", fill = \"skyblue\") +\n  geom_vline(xintercept = upset_rate, color = \"red\") + # Mark the observed upset rate\n  labs(x = \"Proportion of teams that upset a higher seed\", \n       y = \"Count\",\n       title = \"Sampling Distribution Under the Null Hypothesis\")\n\n\n\n\n\n\n\n\nThis plot helps us visualize the null distribution of the data. The red line indicates the upset rate is 0.21875.\nThe p_value is 0.0152, which means we reject the null hypothesis. Our p_value is greater than our level of significance, 0.05. This means that we reject the null hypothesis. The analysis suggests that lower-seeded teams do have upsets, and the proportion of upsets is as large as 10%. From our analysis we determined that the upset rate in the round of 64 in the 2023 March Madness tournament was 21.875% which is much larger than our 10% estimate in the hypothesis.\nData Source:\nhttps://www.kaggle.com/datasets/nishaanamin/march-madness-data/data\nAuthor: NISHAAN AMIN\nThis data was sourced from data during the March Madness tournament. Specifically…\nThe data is pulled from:\n\nhttps://kenpom.com/\nhttps://www.barttorvik.com/#\nhttps://heatcheckcbb.com/\nhttps://abcnews.go.com/538\nhttps://www.espn.com/\nhttps://www.collegepollarchive.com/\nhttps://sports.yahoo.com/\n\nIt was updated about 8 months ago. The data is from 2008 - 2024 for the men’s teams. The year 2020 is not included because the tournament was canceled due to Covid. The first column of almost every dataset displays the year the data is from."
  },
  {
    "objectID": "finalproject.html#overview",
    "href": "finalproject.html#overview",
    "title": "DS002 Projects",
    "section": "Overview",
    "text": "Overview\n\nProject 2: Netflix Analysis\nProject 3: March Madness Permutation Test\nProject 4: WAI Database"
  },
  {
    "objectID": "finalproject.html#netflix-analysis",
    "href": "finalproject.html#netflix-analysis",
    "title": "DS002 Projects",
    "section": "Netflix Analysis",
    "text": "Netflix Analysis\n\nCategorized Netflix titles by genre, used regular expressions to analyze 6 genres (comedy, horror, documentary, drama, action, crime and reality)\nIdentified amount of titles per genre\nExamined Netflix titles original release dates\nDataset pulled from Kaggle, titled Netflix Movies and TV Shows"
  },
  {
    "objectID": "finalproject.html#netflix-analysis---cleaned-up-data",
    "href": "finalproject.html#netflix-analysis---cleaned-up-data",
    "title": "DS002 Projects",
    "section": "Netflix Analysis - Cleaned up Data",
    "text": "Netflix Analysis - Cleaned up Data\n\n\n# A tibble: 5,796 × 2\n   genre  type   \n   &lt;chr&gt;  &lt;chr&gt;  \n 1 Drama  TV Show\n 2 Drama  Movie  \n 3 Horror Movie  \n 4 Action Movie  \n 5 Drama  Movie  \n 6 Drama  TV Show\n 7 Horror Movie  \n 8 Drama  Movie  \n 9 Horror Movie  \n10 Horror Movie  \n# ℹ 5,786 more rows"
  },
  {
    "objectID": "finalproject.html#figure-1---movies-per-genre",
    "href": "finalproject.html#figure-1---movies-per-genre",
    "title": "DS002 Projects",
    "section": "Figure 1 - Movies per Genre",
    "text": "Figure 1 - Movies per Genre"
  },
  {
    "objectID": "finalproject.html#figure-2---number-of-titles-for-tv-shows-and-movies",
    "href": "finalproject.html#figure-2---number-of-titles-for-tv-shows-and-movies",
    "title": "DS002 Projects",
    "section": "Figure 2 - Number of Titles for TV Shows and Movies",
    "text": "Figure 2 - Number of Titles for TV Shows and Movies"
  },
  {
    "objectID": "finalproject.html#key-insights",
    "href": "finalproject.html#key-insights",
    "title": "DS002 Projects",
    "section": "Key Insights",
    "text": "Key Insights\n\nComedy and Drama are the most popular genres.\nCrime and Reality are the least popular genres.\nNotable increase after the early 2000s.\nEarliest title is a TV show that was originally released in 1925."
  },
  {
    "objectID": "finalproject.html#march-madness",
    "href": "finalproject.html#march-madness",
    "title": "DS002 Projects",
    "section": "March Madness",
    "text": "March Madness\n\nConducted a permutation test to analyze upset probabilities in 2023.\nExplored the upset rate for lower-seeded teams in the round of 64.\nNull hypothesis (H0) : the probability that a lower seeded team wins being less than or equal to 90% (Upset win rate ≤ 90%).\nAlternative hypothesis (Ha): the probability that a lower seeded team wins being greater than 90% (Upset win rate &gt; 90%)."
  },
  {
    "objectID": "finalproject.html#march-madness---data",
    "href": "finalproject.html#march-madness---data",
    "title": "DS002 Projects",
    "section": "March Madness - Data",
    "text": "March Madness - Data\n\n\n   YEAR TEAM.NO                   TEAM SEED ROUND POWER.RATING\n1  2023     963                 Purdue    1    64         89.5\n2  2023    1010                Arizona    2    64         89.0\n3  2023     946               Virginia    4    64         83.8\n4  2023     987               Iowa St.    6    64         84.5\n5  2023     956              Texas A&M    7    64         85.1\n6  2023     978                Memphis    8    64         84.9\n7  2023     988                   Iowa    8    64         84.4\n8  2023     991               Illinois    9    64         84.5\n9  2023     945          West Virginia    9    64         84.2\n10 2023     949               Utah St.   10    64         83.5\n11 2023     950                    USC   10    64         83.0\n12 2023    1005              Boise St.   10    64         82.6\n13 2023     964             Providence   11    64         82.8\n14 2023     971     North Carolina St.   11    64         81.4\n15 2023    1009            Arizona St.   11    64         80.3\n16 2023     948                    VCU   12    64         81.7\n17 2023    1000                  Drake   12    64         81.2\n18 2023    1003  College of Charleston   12    64         79.8\n19 2023     968           Oral Roberts   12    64         79.2\n20 2023     989                   Iona   13    64         79.6\n21 2023     983               Kent St.   13    64         79.4\n22 2023     981    Louisiana Lafayette   13    64         76.3\n23 2023     994           Grand Canyon   14    64         75.9\n24 2023     953       UC Santa Barbara   14    64         75.5\n25 2023     973            Montana St.   14    64         75.3\n26 2023     984           Kennesaw St.   14    64         73.3\n27 2023     947                Vermont   15    64         76.3\n28 2023    1004                Colgate   15    64         76.2\n29 2023     951          UNC Asheville   15    64         73.0\n30 2023     970      Northern Kentucky   16    64         72.9\n31 2023     955 Texas A&M Corpus Chris   16    64         69.9\n32 2023     992                 Howard   16    64         69.3\n   POWER.RATING.RANK\n1                 50\n2                 59\n3                205\n4                180\n5                158\n6                164\n7                186\n8                180\n9                194\n10               214\n11               228\n12               246\n13               240\n14               291\n15               318\n16               278\n17               302\n18               329\n19               343\n20               335\n21               340\n22               386\n23               394\n24               400\n25               404\n26               422\n27               386\n28               389\n29               427\n30               428\n31               455\n32               459"
  },
  {
    "objectID": "finalproject.html#outcome-of-upsets",
    "href": "finalproject.html#outcome-of-upsets",
    "title": "DS002 Projects",
    "section": "Outcome of Upsets",
    "text": "Outcome of Upsets\n\nRepresents outcomes in round of 64 within 4 pools, where bottom seed group is all teams seeded from 8-16 and top seed group is all teams seeded 1-8\n\n\n\n# A tibble: 4 × 3\n# Groups:   seed_group [2]\n  seed_group RESULT `n()`\n  &lt;chr&gt;      &lt;chr&gt;  &lt;int&gt;\n1 bottom     loss      25\n2 bottom     win        7\n3 top        loss       7\n4 top        win       25\n\n\nUpset rate (7/32):\n\n\n[1] 0.21875\n\n\nP-Value:\n\n\n[1] 0.035"
  },
  {
    "objectID": "finalproject.html#null-distribution",
    "href": "finalproject.html#null-distribution",
    "title": "DS002 Projects",
    "section": "Null Distribution",
    "text": "Null Distribution"
  },
  {
    "objectID": "finalproject.html#key-insights-1",
    "href": "finalproject.html#key-insights-1",
    "title": "DS002 Projects",
    "section": "Key Insights",
    "text": "Key Insights\n\nUpset rate: 21.9%, much higher than 10%.\nP-value: 0.035 — rejected null hypothesis, less than significance level (0.05).\nLower-seeded teams can achieve significant upsets."
  },
  {
    "objectID": "finalproject.html#wai-database-analysis",
    "href": "finalproject.html#wai-database-analysis",
    "title": "DS002 Projects",
    "section": "WAI Database Analysis",
    "text": "WAI Database Analysis\n\nCompared sex-based differences in absorbency for Aithal (2013).\nData pulled from Aithal study in WAI database"
  },
  {
    "objectID": "finalproject.html#data",
    "href": "finalproject.html#data",
    "title": "DS002 Projects",
    "section": "Data",
    "text": "Data\n\n\n    Identifier Frequency     Sex Mean_Absorbance Unique_Ears\n1  Aithal_2013       250  Female       0.7233140           1\n2  Aithal_2013       250  Female       0.5436131          66\n3  Aithal_2013       250    Male       0.5444056          66\n4  Aithal_2013       250 Unknown       0.6231640           3\n5  Aithal_2013       300  Female       0.5790744           1\n6  Aithal_2013       300  Female       0.5163662          66\n7  Aithal_2013       300    Male       0.5221938          66\n8  Aithal_2013       300 Unknown       0.5825467           3\n9  Aithal_2013       400  Female       0.5225532           1\n10 Aithal_2013       400  Female       0.4348804          66\n11 Aithal_2013       400    Male       0.4434407          66\n12 Aithal_2013       400 Unknown       0.4734422           3\n13 Aithal_2013       500  Female       0.5307322           1\n14 Aithal_2013       500  Female       0.4072390          66\n15 Aithal_2013       500    Male       0.4125729          66\n16 Aithal_2013       500 Unknown       0.4292033           3\n17 Aithal_2013       600  Female       0.4629282           1\n18 Aithal_2013       600  Female       0.4433350          66\n19 Aithal_2013       600    Male       0.4453250          66\n20 Aithal_2013       600 Unknown       0.4624286           3\n21 Aithal_2013       800  Female       0.4296306           1\n22 Aithal_2013       800  Female       0.5140633          66\n23 Aithal_2013       800    Male       0.5132161          66\n24 Aithal_2013       800 Unknown       0.5608810           3\n25 Aithal_2013      1000  Female       0.5344161           1\n26 Aithal_2013      1000  Female       0.6492766          66\n27 Aithal_2013      1000    Male       0.6488667          66\n28 Aithal_2013      1000 Unknown       0.7106753           3\n29 Aithal_2013      1250  Female       0.8438008           1\n30 Aithal_2013      1250  Female       0.7519756          66\n31 Aithal_2013      1250    Male       0.7520200          66\n32 Aithal_2013      1250 Unknown       0.8371913           3\n33 Aithal_2013      1500  Female       0.9016040           1\n34 Aithal_2013      1500  Female       0.7830161          66\n35 Aithal_2013      1500    Male       0.7818892          66\n36 Aithal_2013      1500 Unknown       0.8462829           3\n37 Aithal_2013      2000  Female       0.7489143           1\n38 Aithal_2013      2000  Female       0.7449515          66\n39 Aithal_2013      2000    Male       0.7450622          66\n40 Aithal_2013      2000 Unknown       0.7904039           3\n41 Aithal_2013      2500  Female       0.5603291           1\n42 Aithal_2013      2500  Female       0.5878586          66\n43 Aithal_2013      2500    Male       0.5930597          66\n44 Aithal_2013      2500 Unknown       0.6883140           3\n45 Aithal_2013      3000  Female       0.5569256           1\n46 Aithal_2013      3000  Female       0.5082369          66\n47 Aithal_2013      3000    Male       0.5111097          66\n48 Aithal_2013      3000 Unknown       0.6279454           3\n49 Aithal_2013      4000  Female       0.5518194           1\n50 Aithal_2013      4000  Female       0.4683187          66\n51 Aithal_2013      4000    Male       0.4704967          66\n52 Aithal_2013      4000 Unknown       0.5527944           3\n53 Aithal_2013      5000  Female       0.7667969           1\n54 Aithal_2013      5000  Female       0.6519406          66\n55 Aithal_2013      5000    Male       0.6502550          66\n56 Aithal_2013      5000 Unknown       0.7338458           3\n57 Aithal_2013      6000  Female       0.8838966           1\n58 Aithal_2013      6000  Female       0.7540300          66\n59 Aithal_2013      6000    Male       0.7602814          66\n60 Aithal_2013      6000 Unknown       0.8214159           3\n61 Aithal_2013      8000  Female       0.5207682           1\n62 Aithal_2013      8000  Female       0.6799654          66\n63 Aithal_2013      8000    Male       0.6918249          66\n64 Aithal_2013      8000 Unknown       0.7032796           3"
  },
  {
    "objectID": "finalproject.html#analysis-by-sex-using-the-aithal-2013-study",
    "href": "finalproject.html#analysis-by-sex-using-the-aithal-2013-study",
    "title": "DS002 Projects",
    "section": "Analysis by Sex (using the Aithal (2013) study)",
    "text": "Analysis by Sex (using the Aithal (2013) study)"
  },
  {
    "objectID": "finalproject.html#key-insights-2",
    "href": "finalproject.html#key-insights-2",
    "title": "DS002 Projects",
    "section": "Key Insights",
    "text": "Key Insights\n\nNot a major difference between mean absorbency across frequencies between sexes\nSex does not influence auditory outcomes"
  },
  {
    "objectID": "finalproject.html#questions",
    "href": "finalproject.html#questions",
    "title": "DS002 Projects",
    "section": "Questions?",
    "text": "Questions?"
  }
]