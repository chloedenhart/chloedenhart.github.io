[
  {
    "objectID": "finalproject.html#overview",
    "href": "finalproject.html#overview",
    "title": "DS002 Projects",
    "section": "Overview",
    "text": "Overview\n\nProject 2: Netflix Analysis\nProject 3: March Madness Permutation Test\nProject 4: WAI Database"
  },
  {
    "objectID": "finalproject.html#netflix-analysis",
    "href": "finalproject.html#netflix-analysis",
    "title": "DS002 Projects",
    "section": "Netflix Analysis",
    "text": "Netflix Analysis\n\nCategorized Netflix titles by genre, used regular expressions to analyze 6 genres (comedy, horror, documentary, drama, action, crime and reality)\nIdentified amount of titles per genre\nExamined Netflix titles original release dates\nDataset pulled from Kaggle, titled Netflix Movies and TV Shows"
  },
  {
    "objectID": "finalproject.html#figure-1---movies-per-genre",
    "href": "finalproject.html#figure-1---movies-per-genre",
    "title": "DS002 Projects",
    "section": "Figure 1 - Movies per Genre",
    "text": "Figure 1 - Movies per Genre"
  },
  {
    "objectID": "finalproject.html#figure-2---number-of-titles-for-tv-shows-and-movies",
    "href": "finalproject.html#figure-2---number-of-titles-for-tv-shows-and-movies",
    "title": "DS002 Projects",
    "section": "Figure 2 - Number of Titles for TV Shows and Movies",
    "text": "Figure 2 - Number of Titles for TV Shows and Movies"
  },
  {
    "objectID": "finalproject.html#key-insights",
    "href": "finalproject.html#key-insights",
    "title": "DS002 Projects",
    "section": "Key Insights",
    "text": "Key Insights\n\nComedy and Drama are the most popular genres.\nCrime and Reality are the least popular genres.\nNotable increase after the early 2000s.\nEarliest title is a TV show that was originally released in 1925."
  },
  {
    "objectID": "finalproject.html#march-madness",
    "href": "finalproject.html#march-madness",
    "title": "DS002 Projects",
    "section": "March Madness",
    "text": "March Madness\n\nConducted a permutation test to analyze upset probabilities in 2023.\nExplored the upset rate for lower-seeded teams in the round of 64.\nNull hypothesis (H0) : the probability that a lower seeded team wins being less than or equal to 90% (Upset win rate ≤ 90%).\nAlternative hypothesis (Ha): the probability that a lower seeded team wins being greater than 90% (Upset win rate &gt; 90%)."
  },
  {
    "objectID": "finalproject.html#outcome-of-upsets",
    "href": "finalproject.html#outcome-of-upsets",
    "title": "DS002 Projects",
    "section": "Outcome of Upsets",
    "text": "Outcome of Upsets\n\nRepresents outcomes in round of 64 within 4 pools, where bottom seed group is all teams seeded from 8-16 and top seed group is all teams seeded 1-8\n\n\n\n# A tibble: 4 × 3\n# Groups:   seed_group [2]\n  seed_group RESULT `n()`\n  &lt;chr&gt;      &lt;chr&gt;  &lt;int&gt;\n1 bottom     loss      25\n2 bottom     win        7\n3 top        loss       7\n4 top        win       25\n\n\nUpset rate (7/32):\n\n\n[1] 0.21875\n\n\nP-Value:\n\n\n[1] 0.035"
  },
  {
    "objectID": "finalproject.html#null-distribution",
    "href": "finalproject.html#null-distribution",
    "title": "DS002 Projects",
    "section": "Null Distribution",
    "text": "Null Distribution\n\nHistogram of null distribution with upset rate highlighted."
  },
  {
    "objectID": "finalproject.html#key-insights-1",
    "href": "finalproject.html#key-insights-1",
    "title": "DS002 Projects",
    "section": "Key Insights",
    "text": "Key Insights\n\nUpset rate: 21.9%, much higher than 10%.\nP-value: 0.035 — rejected null hypothesis, less than significance level (0.05).\nLower-seeded teams can achieve significant upsets."
  },
  {
    "objectID": "finalproject.html#wai-database-analysis",
    "href": "finalproject.html#wai-database-analysis",
    "title": "DS002 Projects",
    "section": "WAI Database Analysis",
    "text": "WAI Database Analysis\n\nCompared sex-based differences in absorbency for Aithal (2013).\nData pulled from Aithal study in WAI database"
  },
  {
    "objectID": "finalproject.html#analysis-by-sex-using-the-aithal-2013-study",
    "href": "finalproject.html#analysis-by-sex-using-the-aithal-2013-study",
    "title": "DS002 Projects",
    "section": "Analysis by Sex (using the Aithal (2013) study)",
    "text": "Analysis by Sex (using the Aithal (2013) study)"
  },
  {
    "objectID": "finalproject.html#key-insights-2",
    "href": "finalproject.html#key-insights-2",
    "title": "DS002 Projects",
    "section": "Key Insights",
    "text": "Key Insights\n\nNot a major difference between mean absorbency across frequencies between sexes\nSex does not influence auditory outcomes"
  },
  {
    "objectID": "finalproject.html#questions",
    "href": "finalproject.html#questions",
    "title": "DS002 Projects",
    "section": "Questions?",
    "text": "Questions?"
  },
  {
    "objectID": "miniproject3.html",
    "href": "miniproject3.html",
    "title": "Mini-Project 3",
    "section": "",
    "text": "The code here will provide an analysis on March Madness data. March madness is one of the most unpredictable tournaments in sports, as lower seeded teams are constantly “upsetting” higher seeded teams. The analysis done here will tackle the actual probability of lower seeded teams beating higher seeded teams. Our research question is: What is the probability of a lower-seeded team winning against a higher-seeded team in the first round of the 2023 NCAA March Madness tournament, and does this probability exceed 90%? To determine this I will conduct a permutation test with the hypothesis:\n\nNull hypothesis (H0) : the probability that a lower seeded team wins being less than or equal to 90% (Upset win rate ≤ 90%).\nAlternative hypothesis (Ha): the probability that a lower seeded team wins being greater than 90% (Upset win rate &gt; 90%)\n\nIn this case, we will be using a right tailed permutation test. If the p-value is below the significance level (0.05), we reject the null hypothesis, if it is not, we will fail to reject the null hypothesis.\n\nmarch_madness &lt;- read.csv(\"538 Ratings.csv\")\n\nwalk through these lines of code\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(purrr)\n\n\ntotal_games &lt;- march_madness |&gt;\n  filter(YEAR == 2023, ROUND == 64) |&gt;\n  nrow()\ntotal_games\n\n[1] 32\n\n\n\nupsets &lt;- march_madness |&gt;\n  filter(YEAR == 2023, ROUND &lt; 64, SEED &gt; 8) |&gt;\n  nrow()\nupsets\n\n[1] 7\n\n\n\nmarch_madness |&gt;\n  filter(YEAR == 2023) |&gt;\n  filter(ROUND &lt;= 64) |&gt;\n  mutate(RESULT = ifelse(ROUND &lt; 64, \"win\", \"loss\")) |&gt;\n  mutate(seed_group = ifelse(SEED &lt;=8, \"top\", \"bottom\")) |&gt;\n  group_by(seed_group, RESULT) |&gt;\n  summarize(n())\n\n# A tibble: 4 × 3\n# Groups:   seed_group [2]\n  seed_group RESULT `n()`\n  &lt;chr&gt;      &lt;chr&gt;  &lt;int&gt;\n1 bottom     loss      25\n2 bottom     win        7\n3 top        loss       7\n4 top        win       25\n\n# upset rate = 0.21875, which is 7/32\n\n\n# upset rate drawn from the total number of games in round of 64, and the amount of games won by a team when their seed &gt;= 8. See table above.\nupset_rate &lt;- upsets/total_games\nupset_rate\n\n[1] 0.21875\n\n\n\nrandom_choice &lt;- function(num_teams) {\n  shuffled_seed &lt;- sample(c(\"lower\", \"higher\"), num_teams, replace = TRUE, prob = c(0.9, 0.1))\n  upset_ratio &lt;- mean(shuffled_seed == \"higher\")\n  return(upset_ratio)\n}\n\n\nset.seed(47)\nnum_exper &lt;- 5000\nnull_distribution &lt;- map_dbl(1:num_exper, ~ random_choice(total_games))\n# issue with rendering is that total games is not defined\n\n\np_value &lt;- sum(null_distribution &gt;= upset_rate) / num_exper\np_value\n\n[1] 0.035\n\n\n\nggplot(data.frame(null_distribution), aes(x = null_distribution)) + \n  geom_histogram(bins = 30, color = \"black\", fill = \"skyblue\") +\n  geom_vline(xintercept = upset_rate, color = \"red\") + # red line should be at upset rate not p-value\n  labs(x = \"Proportion of teams that upset a higher seed\", \n       y = \"Count\",\n       title = \"Sampling distribution under the null hypothesis\")\n\n\n\n\n\n\n\n\nThis plot helps us visualize the null distribution of the data. The red line indicates the upset rate is 0.21875.\nThe p_value is 0.0152, which means we reject the null hypothesis. Our p_value is greater than our level of significance, 0.05. This means that we reject the null hypothesis. The analysis suggests that lower-seeded teams do have upsets, and the proportion of upsets is as large as 10%. From our analysis we determined that the upset rate in the round of 64 in the 2023 March Madness tournament was 21.875% which is much larger than our 10% estimate in the hypothesis.\nData Source:\nhttps://www.kaggle.com/datasets/nishaanamin/march-madness-data/data\nAuthor: NISHAAN AMIN\nThis data was sourced from data during the March Madness tournament. Specifically…\nThe data is pulled from:\n\nhttps://kenpom.com/\nhttps://www.barttorvik.com/#\nhttps://heatcheckcbb.com/\nhttps://abcnews.go.com/538\nhttps://www.espn.com/\nhttps://www.collegepollarchive.com/\nhttps://sports.yahoo.com/\n\nIt was updated about 8 months ago. The data is from 2008 - 2024 for the men’s teams. The year 2020 is not included because the tournament was canceled due to Covid. The first column of almost every dataset displays the year the data is from."
  },
  {
    "objectID": "drinks.html",
    "href": "drinks.html",
    "title": "All Drinks",
    "section": "",
    "text": "In this project, I analyze the distribution of drink categories using the TidyTuesday All Drinks dataset. The dataset provides detailed information about various drink types and their associated categories, which makes it ideal for exploring trends and understanding the popularity of different beverages.\n\ndrinks &lt;- read.csv(\"all_drinks.csv\")\n\n\nlibrary(tidyverse)\nlibrary(ggplot2)\n\nggplot(drinks, aes(x = strCategory)) +\n  geom_bar() +\n  labs(\n    x = \"Drink Category\",\n    y = \"Count\",\n    title = \"Analysis of amount of drinks in each drink category\",\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nHere I created a bar plot that visualizes the number of drinks in each category. The x-axis represents the drink categories, while the y-axis displays their respective counts. To enhance readability, I rotated the category labels on the x-axis.\nThis visualization helps identify which drink categories are most common in the dataset and highlights trends that could inform further research or practical applications in food and beverage studies.\nCitation:\nTidyTuesday Project. (2020, February 18). All Drinks Dataset. Retrieved from https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-02-18/food_consumption.csv"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Chloe Denhart",
    "section": "",
    "text": "About this site:\nWelcome to my website! I am a student at Pomona College majoring in Economics and minoring in Data Science. I am a member of the Pomona Woman’s Lacrosse team. Outside of Pomona, I am an avid camper and hiker!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Chloe Denhart",
    "section": "",
    "text": "About this site:\nWelcome to my website! I am a student at Pomona College majoring in Economics and minoring in Data Science. I am a member of the Pomona Woman’s Lacrosse team. Outside of Pomona, I am an avid camper and hiker!"
  },
  {
    "objectID": "food.html",
    "href": "food.html",
    "title": "Food Consumption",
    "section": "",
    "text": "In this project, I analyze data from the TidyTuesday initiative, focusing on food consumption and its associated carbon dioxide (CO2) emissions. The dataset provides insights into the relationship between different types of food consumption and their environmental impact. By visualizing this relationship, I aim to identify patterns and highlight how dietary choices contribute to CO2 emissions, a critical factor in discussions on climate change and sustainability.\n\nfood &lt;- read.csv(\"food_consumption.csv\")\n\nlibrary(tidyverse)\nlibrary(ggplot2)\n\nggplot(food, aes(x = co2_emmission, y = consumption)) +\n  geom_point() +\n  labs(\n    x = \"CO2 Emissions\",\n    y = \"Consumption\",\n    title = \"Relationship food consumption and CO2 emissions\",\n  )\n\n\n\n\n\n\n\n\nUsing the dataset food_consumption.csv, I created a scatter plot that depicts the relationship between food consumption levels (measured in kilograms per capita) and CO2 emissions (measured in kilograms of CO2 equivalent). This plot serves to illustrate whether foods with higher consumption rates also tend to have higher associated CO2 emissions.\nCitations:\nTidyTuesday Project. (2020, May 26). Food Consumption Dataset. Retrieved from https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-05-26/all_drinks.csv"
  },
  {
    "objectID": "miniproject2.html",
    "href": "miniproject2.html",
    "title": "Mini-Project 2",
    "section": "",
    "text": "In this project, I analyzed the Netflix Movies and TV Shows dataset to explore trends and insights into the platform’s content. This dataset, sourced from Kaggle and made available through TidyTuesday, provides information on titles, genres, release years, ratings, and descriptions.\n\nnetflix_titles &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-04-20/netflix_titles.csv')\n\n\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(tidytuesdayR)\nlibrary(stringr)\n\n\nnetflix_titles2 &lt;- netflix_titles |&gt;\n  mutate(listed_in = str_replace_all(listed_in, \"[A-Z]\", tolower))\n\n\ngenre_titles &lt;- netflix_titles2 |&gt;\n  filter(str_detect(listed_in, \"comedy|comedies|drama|horror|documentary|action|crime|reality\")) |&gt;\n  mutate(genre = case_when(\n    str_detect(listed_in, \"comedy|comedies\") ~ \"Comedy\",\n    str_detect(listed_in, \"drama\") ~ \"Drama\",\n    str_detect(listed_in, \"horror\") ~ \"Horror\",\n    str_detect(listed_in, \"documentary\") ~ \"Documentary\",\n    str_detect(listed_in, \"action\") ~ \"Action\",\n    str_detect(listed_in, \"crime\") ~ \"Crime\",\n    str_detect(listed_in, \"reality\") ~ \"Reality\"\n  )) |&gt;\n  select(genre, type)\n\n\ngenre_plot &lt;-ggplot(genre_titles, aes(x = genre)) +\n    geom_bar() +\n    labs(\n      x = \"Genre\",\n      y = \"Count\",\n      title = \"# of Movies per Genre\"\n    )\nprint(genre_plot)\n\n\n\n\n\n\n\n\n\nlibrary(plotly)\n\n# Creating the Plotly interactive plot for genre distribution\ngenre_plotly &lt;- genre_titles |&gt;\n  count(genre) |&gt;\n  plot_ly(x = ~genre, y = ~n, type = 'bar', \n          text = ~paste('Count: ', n), \n          hoverinfo = 'text') |&gt;\n  layout(title = \"Number of Movies per Genre\",\n         xaxis = list(title = \"Genre\"),\n         yaxis = list(title = \"Count\"))\n\n# Display the interactive plot\ngenre_plotly\n\n\n\n\n\n\nyear_counts &lt;- netflix_titles |&gt;\n  group_by(release_year, type) |&gt;\n  summarise(count = n()) |&gt;\n  arrange(desc(count))\n\n\nyear_plot &lt;- \n  ggplot(year_counts, aes(x = release_year , y = count, color = type)) +\n  geom_point() +\n  labs(title = \"Number of Titles by Genre\", \n       x = \"Genre\", \n       y = \"Count\"\n       )\nprint(year_plot)\n\n\n\n\n\n\n\n\n\ndescription_length &lt;- netflix_titles |&gt;\n  mutate(description_length = str_length(description)) |&gt;\n  select(title, description, description_length) |&gt;\n  arrange(desc(description_length))\n\n\nmean_description_length &lt;- description_length |&gt;\n  summarize(mean_length = mean(description_length, na.rm = TRUE))\n\nmean_description_length\n\n# A tibble: 1 × 1\n  mean_length\n        &lt;dbl&gt;\n1        143.\n\n\n\nrating &lt;- netflix_titles |&gt;\n  filter(str_detect(rating, \"^(R|PG-13|PG|G)$\")) |&gt;\n  select(title, rating)\nrating\n\n# A tibble: 1,337 × 2\n   title       rating\n   &lt;chr&gt;       &lt;chr&gt; \n 1 23:59       R     \n 2 9           PG-13 \n 3 21          PG-13 \n 4 187         R     \n 5 3022        R     \n 6 22-Jul      R     \n 7 Æon Flux    PG-13 \n 8 10,000 B.C. PG-13 \n 9 13 Sins     R     \n10 14 Blades   R     \n# ℹ 1,327 more rows\n\n\nAnalysis on Tables/Plots:\nMy first table, I wanted to change the genre descriptions to all lowercase. I did this so that for my next table I could condense all of the genres into six different categories, allowing me to plot a graph showing the amount of movies/television shows in each genre.\nMy first graph represents that amount of movies/television shows in each genre. From this graph, we can see the most common genres are comedy and drama. The least common genres are crime and reality.\nI upgraded my first graph by producing a interactive plot with plotly.\nFrom my third graph we can see the relationship between year, and movie/television show release date. We can infer from this graph that that a lot more movies and television shows were put on Netflix after the mid 2000s.\nI wanted to play around with the str_length function. So I took the length of all of the descriptions of each title. The mean of the description lengths was 143.1004 string characters.\nLastly, I created a new data frame that takes the title of the show and detects the R, PG-13, PG and G ratings\nReference:\nBansal, Shivam. Netflix Movies and TV Shows Dataset. Kaggle.\nDataset available at Netflix Shows Dataset.\nNetflix Shows from Kaggle, contains information about various movies and TV shows available on Netflix. The dataset includes attributes such as the title, genre, language, release year, and rating of each title. It provides a snapshot of Netflix’s catalog and allows for various analyses, such as trends in content release, genre popularity, or the evolution of Netflix’s original content over time."
  },
  {
    "objectID": "miniproject4.html",
    "href": "miniproject4.html",
    "title": "Mini-Project 4",
    "section": "",
    "text": "In this project, I will reproduce the graph created in Figure 1 of the Voss (2020) study using data from the Wideband Acoustic Immittance (WAI) Database. The graph will plot frequency against mean absorption for the 12 studies referenced in Figure 1, showing the differences across studies. I will also explore a second study, Aithal (2013), focusing on differences in mean absorption by sex. Both analyses will involve querying the WAI Database with SQL to extract the necessary data and using R to create plots.\n\nlibrary(RMariaDB)\nlibrary(tidyverse)\nlibrary(ggplot2)\n\n\nSHOW TABLES;\n\n\n7 records\n\n\nTables_in_wai\n\n\n\n\nCodebook\n\n\nMeasurements\n\n\nMeasurements_pre2020\n\n\nPI_Info\n\n\nPI_Info_OLD\n\n\nSubjects\n\n\nSubjects_pre2020\n\n\n\n\n\n\nDESCRIBE Measurements;\n\n\nDisplaying records 1 - 10\n\n\nField\nType\nNull\nKey\nDefault\nExtra\n\n\n\n\nIdentifier\nvarchar(50)\nNO\nPRI\nNA\n\n\n\nSubjectNumber\nint\nNO\nPRI\nNA\n\n\n\nSession\nint\nNO\nPRI\nNA\n\n\n\nEar\nvarchar(50)\nNO\nPRI\n\n\n\n\nInstrument\nvarchar(50)\nNO\nPRI\n\n\n\n\nAge\nfloat\nYES\n\nNA\n\n\n\nAgeCategory\nvarchar(50)\nYES\n\nNA\n\n\n\nEarStatus\nvarchar(50)\nYES\n\nNA\n\n\n\nTPP\nfloat\nYES\n\nNA\n\n\n\nAreaCanal\nfloat\nYES\n\nNA\n\n\n\n\n\n\n\nSELECT *\nFROM Measurements\nLIMIT 0, 5;\n\n\n5 records\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentifier\nSubjectNumber\nSession\nEar\nInstrument\nAge\nAgeCategory\nEarStatus\nTPP\nAreaCanal\nPressureCanal\nSweepDirection\nFrequency\nAbsorbance\nZmag\nZang\n\n\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n210.938\n0.0333379\n113780000\n-0.233504\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n234.375\n0.0315705\n103585000\n-0.235778\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n257.812\n0.0405751\n92951696\n-0.233482\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n281.250\n0.0438399\n86058000\n-0.233421\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n304.688\n0.0486400\n79492800\n-0.232931\n\n\n\n\n\n\nSELECT DISTINCT Identifier\nFROM Measurements;\n\n\nDisplaying records 1 - 10\n\n\nIdentifier\n\n\n\n\nAbur_2014\n\n\nAithal_2013\n\n\nAithal_2014\n\n\nAithal_2014b\n\n\nAithal_2015\n\n\nAithal_2017a\n\n\nAithal_2017b\n\n\nAithal_2019a\n\n\nAithal_2019b\n\n\nAithal_2020a\n\n\n\n\n\n\nSELECT \n  m.Identifier,\n  CONCAT(AuthorsShortList, \" (\", YEAR, \") \", \"N= \", COUNT(DISTINCT SubjectNumber, Ear), \";\", Instrument) AS Label,\n  m.Frequency,\n  AVG(m.Absorbance) AS Mean_Absorbance,\n  COUNT(DISTINCT m.SubjectNumber, m.Ear) AS Unique_Ears\nFROM Measurements AS m\nJOIN PI_Info AS pi\n  ON m.Identifier = pi.Identifier\nWHERE m.Identifier IN ('Abur_2014', 'Werner_2010', 'Voss_2020', 'Feeny_2017', 'Groon_2015', 'Lewis_2015', 'Liu_2008', 'Rosowski_2012', 'Shahnaz_2006', 'Shaver_2013', 'Sun_2016', 'Voss_1994', 'Voss_2010') AND m.Frequency &gt; 200 AND m.Frequency &lt; 8000\nGROUP BY m.Identifier, Frequency, Instrument;\n\n\nlibrary(scales)\n\nears |&gt;\n  ggplot(aes(x = Frequency, y = Mean_Absorbance, color = Label)) +\n  geom_line() +\n  xlim(200, 8000) +\n  ylim(0, 1) +\n  labs(\n    title = \"Mean absorbance from each publication in WAI database\",\n    x = \"Frequency(Hz)\",\n    y = \"Mean Absorbance\",\n    color = NULL\n  ) +\n  # proper x and y scales\n  scale_x_log10(\n    limits = c(200, 8000),\n    breaks = c(200, 400, 600, 800, 1000, 2000, 4000, 6000, 8000)\n  ) +\n  scale_y_continuous(\n    limits = c(0, 1),\n    breaks = seq(0, 1, 0.2)\n  ) +\n  theme_bw() +\n  \n  theme(\n    plot.background = element_rect(\n      fill = \"grey90\", \n      color = \"grey90\", \n      size = 30),\n    legend.position = c(0.01, 0.99),\n    legend.justification = c(0, 1),\n    legend.background = element_rect(\n      color = \"black\",\n      size = 0.3,\n      fill = \"white\"\n    ),\n    legend.text = element_text(size = 7),\n    legend.key.height = unit(0.15, \"mm\"),\n    legend.key = element_rect(size = 0.1),\n    legend.key.size = unit(1, \"cm\"),\n    aspect.ratio = 0.8,\n    text = element_text(face = \"bold\"),\n    plot.title = element_text(hjust = 0.5),\n    plot.margin = margin(12, 12, 12, 12)\n  )\n\n\n\n\n\n\n\n\nThis graphs analyzes 12 different studies from the WAI database. Each line represents data from a specific study, identified by the author, year, instrument, and the number of unique ears measured. The x-axis shows frequency in Hertz (Hz), ranging from 200 Hz to 8000 Hz, while the y-axis displays mean absorption values ranging from 0 to 1. This plot highlights variations in mean absorption across different studies and instruments, demonstrating how measurement techniques and populations may influence auditory outcomes. Each study has differing ranges in mean absorbency and frequency.\nNow we will reproduce a similar graph, finding a study in the same database where subjects of different sex. In doing this I will be using the Aithal (2013) study.\n\nSELECT \n  m.Identifier,\n  m.Frequency,\n  s.Sex,\n  AVG(m.Absorbance) AS Mean_Absorbance,\n  COUNT(DISTINCT m.SubjectNumber, m.Ear) AS Unique_Ears\nFROM Measurements AS m\nJOIN Subjects AS s\n  ON m.SubjectNumber = s.SubjectNumber\nWHERE m.Identifier = 'Aithal_2013'\nGROUP BY m.Frequency, s.Sex\nORDER BY m.Frequency, s.Sex;\n\n\nSELECT Sex, Identifier\nFROM Subjects\n\n\n# Correcting the typo in the 'Sex' column\nsex_analysis &lt;- sex_analysis |&gt; \n  mutate(Sex = str_replace(Sex, \"\\\\bFemake\\\\b\", \"Female\"))\n\n\nsex_analysis |&gt;\n  ggplot(aes(x = Frequency, y = Mean_Absorbance, color = Sex)) +\n  geom_line() +\n  labs(\n    title = \"Frequency vs. Mean Absorbance by Sex for Aithal_2013\",\n    x = \"Frequency (Hz)\",\n    y = \"Mean Absorbance\",\n    color = \"Sex\"\n  ) +\n  scale_x_log10(\n    limits = c(200, 8000),\n    breaks = c(200, 400, 600, 800, 1000, 2000, 4000, 6000, 8000)\n  ) +\n  scale_y_continuous(\n    limits = c(0, 1),\n    breaks = seq(0, 1, 0.2)\n  ) +\n  theme_bw() +\n  \n  theme(\n    plot.background = element_rect(\n      fill = \"grey90\", \n      color = \"grey90\", \n      size = 30),\n    legend.position = c(0.01, 0.99),\n    legend.justification = c(0, 1),\n    legend.background = element_rect(\n      color = \"black\",\n      size = 0.3,\n      fill = \"white\"\n    ),\n    legend.text = element_text(size = 7),\n    legend.key.height = unit(0.15, \"mm\"),\n    legend.key = element_rect(size = 0.1),\n    legend.key.size = unit(1, \"cm\"),\n    aspect.ratio = 0.8,\n    text = element_text(face = \"bold\"),\n    plot.title = element_text(hjust = 0.5),\n    plot.margin = margin(12, 12, 12, 12)\n  )\n\n\n\n\n\n\n\n\nThis graphs analyzes just the Aithal (2013) study from the WAI database. Each line represents data from a specific sex group. There seems to be some mistake in the data where “Female” was spelled “Femake”. The x-axis shows frequency in Hertz (Hz), ranging from 200 Hz to 8000 Hz, while the y-axis displays mean absorption values ranging from 0 to 1. This plot highlights that there are not distinct variations in mean absorption and frequency across different sex’s, demonstrating that sex does not influence auditory outcomes.\nData Source:\nThe data used in this analysis comes from the Wideband Acoustic Immittance (WAI) Database, hosted by Smith College. The WAI Database contains a collection of auditory measurements provide measurements across multiple studies and demographic groups. The database includes data from various peer-reviewed studies, including those focusing on different age, sex, and ethnicity groups.\nCitation:\nVoss:\nVoss, Susan E. Ph.D. Resource Review. Ear and Hearing 40(6):p 1481, November/December 2019. | DOI: 10.1097/AUD.0000000000000790\nAithal:\nAithal, V., Kei, J., Driscoll, C., & Khan, A. (2013). Normative wideband reflectance in healthy neonates: A cross-sectional study. Journal of the American Academy of Audiology, 24(9), 832-842."
  }
]